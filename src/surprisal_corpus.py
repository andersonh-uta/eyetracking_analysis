"""
Iterate through some corpora and get the syntactic parses for each
sentence within.  Store these as text files of space-separated numbers,
where the ith number is for the ith non-punctuation, non-whitespace token,
and its value indicates the index of that token's syntactic head.
"""

import bz2
from collections import defaultdict, Counter
import os
import pickle
import random
import re
import json

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from scipy.spatial.distance import cosine
from scipy.stats import beta
import spacy
from tqdm import tqdm

# NLP = spacy.load("en_core_web_lg", disable=["ner"])

def reddit(reddit_dir=r"C:\Users\andersonh\Documents\Grad School\Ling 5347 - Pragmatics - Final Project\reddit"):
    """
    Scan through the Reddit corpus and generate the various metrics
    to use for calculating surprisal.

    :param reddit_dir: str; path to the directory storing reddit files.
    :return: yields post texts.
    """
    files = list(os.walk(reddit_dir))
    files = [
        f"{i[0]}\\{j}"
        for i in files
        for j in i[-1]
        if j.endswith(".bz2")
    ]

    yield from (
        k
        for j in tqdm(files, desc="Files", position=1)
        for i in bz2.open(j, "rt")
        for k in NLP(json.loads(i)["body"]).sents
    )

def parse_docs(corpus, out_dir="../Corpus Surprisal Data"):
    """
    Generate surprisal stimuli data streams for each of the

    :param corpus: iterable of parsed spaCy sentences.
    :return: writes parses to file.
    """
    # some variables to make code lines shorter
    # POS = fine grained POS tags
    # LEM = coarse-grained POS tags
    # DEP = dependency relation
    # LEM = lemmas, or "OOV" if word is out of vocabulary
    # SIM = cosine similarities of the preceding window to the current token.
    if not os.path.isdir(out_dir):
        os.makedirs(out_dir)

    regex = re.compile(r"\s+")

    with open(f"{out_dir}/pos.txt", "w", encoding="utf8") as POS, \
        open(f"{out_dir}/tag.txt", "w", encoding="utf8") as TAG,  \
        open(f"{out_dir}/dep.txt", "w", encoding="utf8") as DEP,  \
        open(f"{out_dir}/lem.txt", "w", encoding="utf8") as LEM,  \
        open(f"{out_dir}/sim-5.txt", "w", encoding="utf8") as SIM_5, \
        open(f"{out_dir}/sim-tot.txt", "w", encoding="utf8") as SIM_TOT:

        for S in tqdm(corpus, unit_scale=True, desc="Parsing"):
            if len(S) <= 10: continue
            # print(S)
            # exit()
            POS.write(" ".join(regex.sub(" ", i.pos_) for i in S) + "\n")
            TAG.write(" ".join(regex.sub(" ", i.tag_) for i in S) + "\n")
            DEP.write(" ".join(regex.sub(" ", i.dep_) for i in S) + "\n")
            LEM.write(" ".join(regex.sub(" ", i.lemma_) for i in S if not i.is_space) + "\n")
            SIM_TOT.write(
                " ".join(
                    str(cosine(S[:i].vector, S[i].vector))
                    for i in range(1, len(S))
                ) + "\n"
            )
            SIM_5.write(
                " ".join(
                    str(cosine(S[max(0, i-5):i].vector, S[i].vector))
                    for i in range(1, len(S))
                ) + "\n"
            )

def get_surprisal_discreet(infile, window=5):
    """
    Read a parsed file generated by parse_docs, and
    generate a basic markov chain to model the probabilities.

    This version only works with discreet metrics, i.e. non-similarity
    metrics.

    :param infile: str; path to the file.
    :return:
    """
    # markov = defaultdict(Counter)
    markov = defaultdict(lambda: defaultdict(int))
    desc = f"Surprisal: {os.path.basename(infile)}"
    tokenizer = re.compile("\s+")
    with open(infile, "r", encoding="utf8") as F:
        ngrams = (
            tokenizer.split(i.strip())
            for i in tqdm(F.readlines(), desc=desc, unit_scale=True, unit=" sentences")
        )

        ngrams = (
            (" ".join(toks[i:i+window]), (toks[i+window],))
            for toks in ngrams
            for i in range(0, len(toks) - window)
            if len(toks) >= 10
        )

        for N in ngrams:
            markov[N[1]][N[0]] += 1

    # Convert to a Pandas DataFrame for a more consistent I/O.
    # print(markov.keys())
    print("dictionarying")
    markov = {i:dict(markov[i]) for i in markov.keys()}
    print("dataframing")
    df = pd.DataFrame(markov, dtype=int)

    # Filter rows with very few counts.  We'll use the threshold
    # of window ** log10(vocab).  This ensures that
    # anything occuring "more or less at chance" is removed, so that
    # the probabilities for those entries can later be flattened.
    total = df.sum()
    vocab = df.shape[1]
    print("Applying threshold counts.")
    to_remove = np.nansum(df.values, axis=1) >= (window ** np.log(vocab))
    print("np.where()...")
    to_remove = np.where(to_remove)[0]
    print("TOTAL ROWS:", df.shape[0])
    print("KEPT ROWS: ", len(to_remove))

    df.iloc[to_remove].to_csv(f"../out/{os.path.basename(infile)[:-4]}.csv.bz2", compression="bz2")

def get_surprisal_continuous(infile):
    """
    Generates a continuous probability distribution
    for the surprisal metrics.

    :param infile: str; path to file containing sequences
     of discreet values.
    :return:
    """
    with open(infile, "r", encoding="utf8") as F:
        sims = np.array([
            j
            for i in tqdm(F.readlines(), unit_scale=True, desc=os.path.basename(infile))
            for j in np.fromstring(i.replace("nan", "").strip(), sep=" ", dtype=np.float32)
            if i.strip()
        ])
        sims = np.clip(sims, 0, 1)
        print(sims.shape)
        print("Fitting beta distribution to data.")
        sims = np.random.choice(sims, size=20_000_000, replace=False)
        B = beta.fit(sims)
        print(B)
        with open(f"../out/{os.path.basename(infile)}", "w", encoding="utf8") as F:
            F.write(f"beta distribution\n{B}")

    return 0

def main():
    parse_docs(reddit())
    get_surprisal_discreet("../Corpus Surprisal Data - 45m processed/dep.txt")
    get_surprisal_discreet("../Corpus Surprisal Data - 45m processed/tag.txt")
    get_surprisal_discreet("../Corpus Surprisal Data - 45m processed/pos.txt")
    get_surprisal_continuous("../Corpus Surprisal Data - 45m processed/sim-5.txt")
    get_surprisal_continuous("../Corpus Surprisal Data - 45m processed/sim-tot.txt")